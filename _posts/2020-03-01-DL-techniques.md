---
title: Deep Learning techniques
tags:
- Keras
- DL
---
<!--more-->
## keras中loss与val_loss的关系

loss是训练集的损失值，val_loss是测试集的损失值

以下是loss与val_loss的变化反映出训练走向的规律总结：

train loss 不断下降，test loss不断下降，说明网络仍在学习;（最好的）

train loss 不断下降，test loss趋于不变，说明网络过拟合;（max pool或者正则化）

train loss 趋于不变，test loss不断下降，说明数据集100%有问题;（检查dataset）

train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;（减少学习率）

train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。（最不好的情况）
## caffe深度学习进行迭代的时候loss曲线开始震荡原因

###训练的batch\_size太小

1.  当数据量足够大的时候可以适当的减小batch\_size,由于数据量太大，内存不够。但盲目减少会导致无法收敛，batch\_size=1时为在线学习。

2.  batch的选择，首先决定的是下降方向，如果数据集比较小，则完全可以采用全数据集的形式。这样做的好处有两点，

    1）全数据集的方向能够更好的代表样本总体，确定其极值所在。

    2）由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。

3.  增大batchsize的好处有三点：

    1）内存的利用率提高了，大矩阵乘法的并行化效率提高。

    2）跑完一次epoch(全数据集)所需迭代次数减少，对于相同的数据量的处理速度进一步加快。

    3）一定范围内，batchsize越大，其确定的下降方向就越准，引起训练震荡越小。

4.  盲目增大的坏处：

 1）当数据集太大时，内存撑不住。

    2）batchsize增大到一定的程度，其确定的下降方向已经基本不再变化。

总结：

        1）batch数太小，而类别又比较多的时候，可能会导致loss函数震荡而不收敛，尤其是在你的网络比较复杂的时候。

        2）随着batchsize增大，处理相同的数据量的速度越快。

        3）随着batchsize增大，达到相同精度所需要的epoch数量越来越多。

        4）由于上述两种因素的矛盾， Batch\_Size 增大到某个时候，达到时间上的最优。

        5）过大的batchsize的结果是网络很容易收敛到一些不好的局部最优点。同样太小的batch也存在一些问题，比如训练速度很慢，训练不容易收敛等。

        6）具体的batch size的选取和训练集的样本数目相关
